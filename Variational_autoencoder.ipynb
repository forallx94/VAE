{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('rspirit36_tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ac3687066fad88b1005b1d692a48a96079325f9e924ee066cc0b71671557d05"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "VAE : https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Flatten\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "m = 50\n",
    "n_z = 2\n",
    "n_epoch = 10\n",
    "\n",
    "\n",
    "# Q(z|X) -- encoder\n",
    "inputs = Input(shape=(784,))\n",
    "h_q = Dense(512, activation='relu')(inputs)\n",
    "mu = Dense(n_z, activation='linear')(h_q)\n",
    "log_sigma = Dense(n_z, activation='linear')(h_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(args):\n",
    "    mu, log_sigma = args\n",
    "    eps = K.random_normal(shape=(m, n_z), mean=0., stddev=1.)\n",
    "    return mu + K.exp(log_sigma / 2) * eps\n",
    "\n",
    "\n",
    "# Sample z ~ Q(z|X)\n",
    "z = Lambda(sample_z)([mu, log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(X|z) -- decoder\n",
    "decoder_hidden = Dense(512, activation='relu')\n",
    "decoder_out = Dense(784, activation='sigmoid')\n",
    "\n",
    "h_p = decoder_hidden(z)\n",
    "outputs = decoder_out(h_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall VAE model, for reconstruction and training\n",
    "vae = Model(inputs, outputs)\n",
    "\n",
    "# Encoder model, to encode input into latent variable\n",
    "# We use the mean as the output as it is the center point, the representative of the gaussian\n",
    "encoder = Model(inputs, mu)\n",
    "\n",
    "# Generator model, generate new data given latent variable z\n",
    "d_in = Input(shape=(n_z,))\n",
    "d_h = decoder_hidden(d_in)\n",
    "d_out = decoder_out(d_h)\n",
    "decoder = Model(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_true, y_pred):\n",
    "    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "    # E[log P(X|z)]\n",
    "    recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "    kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)\n",
    "\n",
    "    return recon + kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 427.4171 - accuracy: 0.0092\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 427.4169 - accuracy: 0.0097\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 427.4168 - accuracy: 0.0099\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 427.4167 - accuracy: 0.0101\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 427.4167 - accuracy: 0.0097\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 427.4167 - accuracy: 0.0095\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 427.4166 - accuracy: 0.0094\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 108us/step - loss: 427.4166 - accuracy: 0.0095\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 107us/step - loss: 427.4166 - accuracy: 0.0094\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 427.4166 - accuracy: 0.0097\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f0cecbc940>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "x_train = x_train.reshape((-1,784))\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss, metrics=['accuracy'])\n",
    "vae.fit(x_train, x_train, batch_size=m, nb_epoch=n_epoch)"
   ]
  },
  {
   "source": [
    "VAE : https://keraskorea.github.io/posts/2018-10-23-keras_autoencoder/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 28 * 28\n",
    "intermediate_dim = 64\n",
    "latent_dim = 2\n",
    "\n",
    "inputs = keras.Input(shape=(original_dim,))\n",
    "h = layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_sigma = layers.Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "# Create decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = keras.Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss = keras.losses.binary_crossentropy(inputs, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "('Error when checking model target: expected no data, but got:', array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-40e565008387>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\rspirit36_tensorflow2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rspirit36_tensorflow2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\rspirit36_tensorflow2\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m     68\u001b[0m             raise ValueError('Error when checking model ' +\n\u001b[0;32m     69\u001b[0m                              \u001b[0mexception_prefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                              'expected no data, but got:', data)\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('Error when checking model target: expected no data, but got:', array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "vae.fit(x_train, x_train, epochs=100, batch_size=32, validation_data = (x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}